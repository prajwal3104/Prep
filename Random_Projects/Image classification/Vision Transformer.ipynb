{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\" \n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "#from keras.backend import ops\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
      "169001437/169001437 [==============================] - 272s 2us/step\n",
      "x_train shape: (50000, 32, 32, 3) y_train shape: (50000, 1)\n",
      "x_test shape: (10000, 32, 32, 3) y_test shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "classes = 100\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape}\", f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape}\", f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "image_size = 72\n",
    "patch_size = 6 #this is the size of the patches extracted from the images, help in the generation of the embeddings and the attention maps\n",
    "num_patches = (image_size // patch_size) ** 2 #this is the number of patches extracted from the images, help in the generation of the embeddings and the attention maps\n",
    "projection_dim = 64 #this is the dimension of the embeddings\n",
    "num_heads = 4 #this is the number of heads in the multi-head attention\n",
    "transformer_units = [\n",
    "    projection_dim * 2, #this is the size of the transformer layers\n",
    "    projection_dim, #this is the size of the transformer layers\n",
    "]  # Size of the transformer layers\n",
    "\n",
    "mlp_head_units = [2048, 1024] #this is the size of the MLP head\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Data augmentation?\n",
    "\n",
    ">Ans: Data augmentation is a technique to artificially create new training data from existing training data. This is done by applying domain-specific techniques to examples from the training data that create new and different training examples. This means that the training dataset can be artificially expanded by creating transformed versions of images in the dataset. Training deep learning neural network models on more data can result in more skillful models, and the augmentation techniques can create variations of the images that can improve the ability of the fit models to generalize what they have learned to new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(), #this is the normalization layer\n",
    "        layers.Resizing(image_size, image_size), #we resize the images to the size of the patches\n",
    "        layers.RandomFlip(\"horizontal\"), #we flip the images horizontally\n",
    "        layers.RandomRotation(factor=0.02), #random rotation of the images\n",
    "        layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2 #we do this to zoom in and out of the images\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\", #naming the layer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Multi-layer perceptron?\n",
    "\n",
    ">Ans: MLP (multi-layer perceptron) is a feedforward artificial neural network that has one or more hidden layers between the input and output layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate): #this is the MLP layer\n",
    "    for units in hidden_units: #we iterate through the hidden units\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x) #we use the GELU activation function\n",
    "        x = layers.Dropout(dropout_rate)(x) #we use dropout to prevent overfitting\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### why we need to reshape the patches?\n",
    ">Ans: we need to reshape the patches because we need to flatten the patches to feed them into the transformer encoder, the transformer encoder takes a 2D tensor as input, so we need to flatten the patches to feed them into the transformer encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size): \n",
    "        super(Patches, self).__init__() #we call the super class\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images): \n",
    "        input_shape = tf.shape(images) \n",
    "        batch_size = input_shape[0]\n",
    "        hight = input_shape[1]\n",
    "        width = input_shape[2] \n",
    "        num_channels = input_shape[3] #this is the number of channels of the images\n",
    "        num_patches_h = hight // self.patch_size \n",
    "        num_patches_w = width // self.patch_size \n",
    "        patches = keras.image.extrat_patches(images, size = self.patch_size)\n",
    "        patches = tf.reshape(\n",
    "            patches, \n",
    "                    (\n",
    "                        batch_size, \n",
    "                        num_patches_h * num_patches_w, \n",
    "                        self.patch_size * self.patch_size * num_channels\n",
    "                    )\n",
    "                ),\n",
    "        return patches\n",
    "    \n",
    "    def get_config(self): #this is used to get the configuration of the layer\n",
    "        config = super(Patches, self).get_config()\n",
    "        config.update({\"patch_size\": self.patch_size})\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explaining above code in brief.\n",
    "\n",
    "1. we get the shape of the images\n",
    "2. we get the batch size\n",
    "3. we get the height of the images\n",
    "4. we get the width of the images\n",
    "5. this is the number of channels of the images\n",
    "6. this is used to get the number of patches in the height\n",
    "7. this is used to get the number of patches in the width\n",
    "8. we reshape the patches\n",
    "9. we return the patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
    "plt.imshow(image.astype(\"uint8\"))\n",
    "plt.axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
